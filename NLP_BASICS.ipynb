{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu1hk8qRvd8neJZEc59cp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshaelon/NLP_Basics/blob/main/NLP_BASICS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EgMvBS0pK1-",
        "outputId": "3ac2bcef-990d-42d0-fd0e-8f804331496a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBomBLWEoQ-j",
        "outputId": "cd5c2e9f-4ac5-4012-96b8-097e8494d59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens :n ['hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Word tokenization\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#sample sentence\n",
        "text = \"hello,how are you doing today?\"\n",
        "\n",
        "\n",
        "#tojenize the text\n",
        "tokens=word_tokenize(text)\n",
        "\n",
        "print(\"Tokens :n\",tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Senetence Tokenization"
      ],
      "metadata": {
        "id": "JNg3TFywpSE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"hello, i am cristiano ronaldo, i am a real madrid player, i am the greatest goalscorer of football.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Tokenized sentences :\\n\",sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Pif4FOopa1",
        "outputId": "680a1e25-863a-4558-b496-a3adc0cc8e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences :\n",
            " ['hello, i am cristiano ronaldo, i am a real madrid player, i am the greatest goalscorer of football.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal"
      ],
      "metadata": {
        "id": "-QrpDatkqSe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# updating the stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text=\"This is a sample sentence, showing the use of stop words filtration.\"\n",
        "\n",
        "#tokenize the words\n",
        "\n",
        "words=word_tokenize(text)\n",
        "\n",
        "#get the stop words in English\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "#Filtering the stop words\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print('Filtered Tokens :\\n',filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbLy926ypoic",
        "outputId": "606d1f29-8655-4a02-8320-589e4a91a9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens :\n",
            " ['sample', 'sentence', ',', 'showing', 'use', 'stop', 'words', 'filtration', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Another language"
      ],
      "metadata": {
        "id": "azp-xELystI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1=\"vers la lemur\"\n",
        "\n",
        "words=word_tokenize(text1)\n",
        "\n",
        "stop_words=set(stopwords.words('french'))\n",
        "\n",
        "flitered_words=[word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print('Filtered tokens are :\\n',filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6FGOI8irEsH",
        "outputId": "40d5ea9a-a716-4bc6-8afd-f0bab94d4647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tokens are :\n",
            " ['sample', 'sentence', ',', 'showing', 'use', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming :\n",
        "Reducing the words into their root words\n",
        "\n",
        "ex: running --> ran --> run\n",
        "2. going --> gone ---> go\n",
        "\n",
        "\n",
        "the disadvantage is the words that are given ouput might not be real words, but their cannonical forms."
      ],
      "metadata": {
        "id": "r6AvGQ7GtWfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming (Porter Stemer)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "words=['running','ran','runs','easily','fairly']\n",
        "\n",
        "porter_stems=[stemmer.stem(word) for word in words]\n",
        "\n",
        "print('Porter stemmer output:\\n',porter_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yTK1HbvtVtB",
        "outputId": "3a70a3ff-6a30-4386-cf01-455a929f2ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmer output:\n",
            " ['run', 'ran', 'run', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "POm-ZOz-yJ9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "words=['running','ran','runs','easily','fairly']\n",
        "\n",
        "\n",
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print('Lemmatizer output:\\n',lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KsrPVXqtLyG",
        "outputId": "f0849e53-517e-403a-e408-b27d4815ab70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatizer output:\n",
            " ['running', 'ran', 'run', 'easily', 'fairly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output, no word has been changed apart from the 'RUNS'."
      ],
      "metadata": {
        "id": "iUfSNRHyyuMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parts of speech tagging"
      ],
      "metadata": {
        "id": "mnPBZojY2kLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text=\"the quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "#tokenize\n",
        "tokens=nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "#perform pos tagging\n",
        "\n",
        "pos_tags=nltk.pos_tag(tokens)\n",
        "\n",
        "print('POS Tags:',pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MSKqH5EyiMO",
        "outputId": "e42ab5a9-7f1a-4657-cead-84d5aea1594b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents=[\n",
        " \"the quick brown fox\",\n",
        " \"the quick brown fox jumps over the lazy dog\",\n",
        " \"the lazy dog\"\n",
        "]\n",
        "\n",
        "vectorizer=TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix=vectorizer.fit_transform(documents)\n",
        "\n",
        "print('TF-IDF Matrix:\\n',tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2UF_Kgs3ByU",
        "outputId": "a026033f-46cf-4b30-9ee7-29218ba11b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            " [[0.52682017 0.         0.52682017 0.         0.         0.\n",
            "  0.52682017 0.40912286]\n",
            " [0.30330642 0.30330642 0.30330642 0.3988115  0.30330642 0.3988115\n",
            "  0.30330642 0.47108899]\n",
            " [0.         0.61980538 0.         0.         0.61980538 0.\n",
            "  0.         0.48133417]]\n"
          ]
        }
      ]
    }
  ]
}